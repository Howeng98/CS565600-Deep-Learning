{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nthu-datalab.github.io/ml/labs/13-2_Image-Caption/13-2_Image-Caption.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-16 17:08:30.712871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-16 17:08:30.713213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-16 17:08:30.894900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-16 17:08:30.895233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-16 17:08:30.895531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-16 17:08:30.895831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-16 17:08:30.896805: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-16 17:08:30.897515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-16 17:08:30.897834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-16 17:08:30.898159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-16 17:08:31.353550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-16 17:08:31.353837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-16 17:08:31.354033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-16 17:08:31.354219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7371 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 16 17:08:31 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   45C    P2    45W / 250W |    248MiB /  8119MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
      "|  0%   45C    P8    11W / 250W |      8MiB /  8119MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1212      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    0   N/A  N/A      1413      G   /usr/bin/gnome-shell                2MiB |\n",
      "|    0   N/A  N/A   1112978      C   /bin/python3                      231MiB |\n",
      "|    1   N/A  N/A      1212      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = './words_captcha/'\n",
    "annotation_file = './words_captcha/spec_train_val.txt'\n",
    "\n",
    "# read spec_train_val.txt file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# initial variable\n",
    "train_img_name = []\n",
    "val_img_name = []\n",
    "train_annotation = []\n",
    "val_annotation = []\n",
    "num = 0\n",
    "\n",
    "# first 100000 data are training set, others are validation (20000)\n",
    "for line in lines:\n",
    "    line = line.strip('\\n')\n",
    "    line = line.split(' ')\n",
    "    if num<100000:\n",
    "        train_img_name.append(line[0])\n",
    "        train_annotation.append(line[1])\n",
    "    else:\n",
    "        val_img_name.append(line[0])\n",
    "        val_annotation.append(line[1])\n",
    "    num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 20000\n"
     ]
    }
   ],
   "source": [
    "# print len and first data of both train & val data, to check if data is split successfully\n",
    "print(len(train_img_name), len(val_img_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def cal_max_length(annotations):\n",
    "    max_len = 0\n",
    "    for annotation in annotations:\n",
    "        if len(annotation) > max_len:\n",
    "            max_len = len(annotation)\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_to_idx = {}\n",
    "idx_to_character = {}\n",
    "character_to_idx['<pad>'] = 0\n",
    "idx_to_character[0] = '<pad>'\n",
    "index = 1\n",
    "\n",
    "for annotation in (train_annotation):\n",
    "    for character in annotation:\n",
    "        if character not in character_to_idx:\n",
    "            character_to_idx[character] = index\n",
    "            idx_to_character[index] = character\n",
    "            index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_to_idx['<start>'] = 27\n",
    "idx_to_character[27] = '<start>'\n",
    "\n",
    "character_to_idx['<end>'] = 28\n",
    "idx_to_character[28] = '<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotation_idx = []\n",
    "val_annotation_idx = []\n",
    "\n",
    "# Find out the max_length\n",
    "max_len = cal_max_length(train_annotation) + 2\n",
    "max_len\n",
    "\n",
    "for annotation in train_annotation:\n",
    "    annotation_idx = [27]\n",
    "    for character in annotation:\n",
    "        annotation_idx.append(character_to_idx[character])\n",
    "    annotation_idx.append(28)\n",
    "    while len(annotation_idx) < max_len:\n",
    "        annotation_idx.append(0)\n",
    "    train_annotation_idx.append(annotation_idx)\n",
    "    \n",
    "for annotation in val_annotation:\n",
    "    annotation_idx = [27]\n",
    "    for character in annotation:\n",
    "        annotation_idx.append(character_to_idx[character])\n",
    "    annotation_idx.append(28)\n",
    "    while len(annotation_idx) < max_len:\n",
    "        annotation_idx.append(0)\n",
    "    val_annotation_idx.append(annotation_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these parameters according to your system's configuration\n",
    "BATCH_SIZE = 8\n",
    "BUFFER_SIZE = 5000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "# num_steps = len(img_name_train) // BATCH_SIZE\n",
    "vocab_size = len(character_to_idx)\n",
    "num_steps = len(train_img_name) // BATCH_SIZE\n",
    "val_num_steps = len(val_img_name) // BATCH_SIZE\n",
    "\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n",
    "\n",
    "LEARNING_RATE = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_name, annotation):\n",
    "    img = tf.io.read_file(IMAGE_DIR + image_name + '.png')\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # resize height to 300, width to 160\n",
    "    img = tf.image.resize(img, (300, 160))\n",
    "    img = img/255 - 1.\n",
    "    return img, annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_img_name,train_annotation_idx))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_img_name,val_annotation_idx))\n",
    "\n",
    "train_dataset = train_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(200)\n",
    "\n",
    "val_dataset = val_dataset.shuffle(BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.prefetch(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 300, 160, 3), (None, 7)), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class conv_leaky_relu(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, size, stride):\n",
    "        super(conv_leaky_relu, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(filters, size, stride, padding=\"same\",\n",
    "                      kernel_initializer=tf.keras.initializers.TruncatedNormal())\n",
    "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "        self.lkrelu = tf.keras.layers.LeakyReLU(0.1)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.conv(inputs)\n",
    "        x = self.batchnorm(x,training = training)\n",
    "        x = self.lkrelu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Extracter(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Feature_Extracter, self).__init__()\n",
    "        self.cr1 = conv_leaky_relu(64,3,1)\n",
    "        self.cr2 = conv_leaky_relu(64,3,1)\n",
    "        self.max_pooling1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr3 = conv_leaky_relu(128,3,1)\n",
    "        self.cr4 = conv_leaky_relu(128,3,1)\n",
    "        self.max_pooling2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr5 = conv_leaky_relu(256,3,1)\n",
    "        self.cr6 = conv_leaky_relu(256,3,1)\n",
    "        self.cr7 = conv_leaky_relu(256,3,1)\n",
    "        self.cr8 = conv_leaky_relu(256,3,1)\n",
    "        self.max_pooling3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr9 = conv_leaky_relu(512,3,1)\n",
    "        self.cr10 = conv_leaky_relu(512,3,1)\n",
    "        self.cr11 = conv_leaky_relu(512,3,1)\n",
    "        self.cr12 = conv_leaky_relu(512,3,1)\n",
    "        self.max_pooling4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr13 = conv_leaky_relu(512,3,1)\n",
    "        self.cr14 = conv_leaky_relu(512,3,1)\n",
    "        self.cr15 = conv_leaky_relu(512,3,1)\n",
    "        self.cr16 = conv_leaky_relu(512,3,1)\n",
    "        self.max_pooling5 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr17 = conv_leaky_relu(1024,3,1)\n",
    "        self.cr18 = conv_leaky_relu(1024,3,1)\n",
    "        self.cr19 = conv_leaky_relu(1024,3,1)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.cr1(inputs,training)\n",
    "        x = self.cr2(x,training)\n",
    "        x = self.max_pooling1(x)\n",
    "        x = self.cr3(x,training)\n",
    "        x = self.cr4(x,training)\n",
    "        x = self.max_pooling2(x)\n",
    "        x = self.cr5(x,training)\n",
    "        x = self.cr6(x,training)\n",
    "        x = self.cr7(x,training)\n",
    "        x = self.cr8(x,training)\n",
    "        x = self.max_pooling3(x)\n",
    "        x = self.cr9(x,training)\n",
    "        x = self.cr10(x,training)\n",
    "        x = self.cr11(x,training)\n",
    "        x = self.cr12(x,training)\n",
    "        x = self.max_pooling4(x)\n",
    "        x = self.cr13(x,training)\n",
    "        x = self.cr14(x,training)\n",
    "        x = self.cr15(x,training)\n",
    "        x = self.cr16(x,training)\n",
    "        x = self.max_pooling5(x)\n",
    "        x = self.cr17(x,training)\n",
    "        x = self.cr18(x,training)\n",
    "        x = self.cr19(x,training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"feature__extracter\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_leaky_relu (conv_leaky  multiple                 2048      \n",
      " _relu)                                                          \n",
      "                                                                 \n",
      " conv_leaky_relu_1 (conv_lea  multiple                 37184     \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  multiple                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_leaky_relu_2 (conv_lea  multiple                 74368     \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " conv_leaky_relu_3 (conv_lea  multiple                 148096    \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  multiple                 0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv_leaky_relu_4 (conv_lea  multiple                 296192    \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " conv_leaky_relu_5 (conv_lea  multiple                 591104    \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " conv_leaky_relu_6 (conv_lea  multiple                 591104    \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " conv_leaky_relu_7 (conv_lea  multiple                 591104    \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  multiple                 0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv_leaky_relu_8 (conv_lea  multiple                 1182208   \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " conv_leaky_relu_9 (conv_lea  multiple                 2361856   \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " conv_leaky_relu_10 (conv_le  multiple                 2361856   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_11 (conv_le  multiple                 2361856   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  multiple                 0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv_leaky_relu_12 (conv_le  multiple                 2361856   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_13 (conv_le  multiple                 2361856   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_14 (conv_le  multiple                 2361856   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_15 (conv_le  multiple                 2361856   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  multiple                 0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv_leaky_relu_16 (conv_le  multiple                 4723712   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_17 (conv_le  multiple                 9442304   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_18 (conv_le  multiple                 9442304   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43,654,720\n",
      "Trainable params: 43,637,568\n",
      "Non-trainable params: 17,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extracter = Feature_Extracter()\n",
    "feature_extracter.build((None,300,160,3))\n",
    "feature_extracter.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train_vgg19\"\n",
    "ckpt = tf.train.Checkpoint(feature_extracter=feature_extracter,\n",
    "                           encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([character_to_idx['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = feature_extracter(img_tensor,True)\n",
    "        features = tf.reshape(features,(features.shape[0], -1, features.shape[3]))\n",
    "        features = encoder(features)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = feature_extracter.trainable_variables + encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        print ('Epoch {} {}/{} Train Loss {:.6f}'.format(epoch + 1,batch+1,num_steps,total_loss/(batch+1)),end='\\r')\n",
    "    print('')\n",
    "    equal_num = 0\n",
    "    total_val_loss = 0\n",
    "    for (batch, (img_tensor, target)) in enumerate(val_dataset):\n",
    "        val_loss = 0\n",
    "        hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "        dec_input = tf.expand_dims([character_to_idx['<start>']]*BATCH_SIZE, 1)\n",
    "        features = feature_extracter(img_tensor,False)\n",
    "        features = tf.reshape(features,(features.shape[0], -1, features.shape[3]))\n",
    "        features = encoder(features)\n",
    "        result = np.full((BATCH_SIZE, 1), 27)\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            predicted_id = tf.argmax(predictions,axis=1).numpy()\n",
    "            val_loss += loss_function(target[:, i], predictions)\n",
    "            result = np.concatenate((result, predicted_id.reshape((BATCH_SIZE,1))), axis=1)\n",
    "            dec_input = tf.expand_dims(predicted_id, 1)\n",
    "        target_array = target.numpy()\n",
    "        total_val_loss += (val_loss / int(target.shape[1]))\n",
    "        for i in range(BATCH_SIZE):\n",
    "            for j in range(max_len):\n",
    "                if result[i][j] == 28 and target_array[i][j] == 28:\n",
    "                    if (result[i][1:j] == target_array[i][1:j]).all():\n",
    "                        equal_num+=1\n",
    "                    break\n",
    "        print ('Validation Accuracy {:.6f}, Validation Loss {:.6f}'.format(float(equal_num)/((batch+1)*BATCH_SIZE),total_val_loss/(batch+1)),end='\\r')\n",
    "    \n",
    "    print('')\n",
    "\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    ckpt_manager.save()\n",
    "    output_string = 'Epoch {} Train Loss {:.6f} Validation Accuracy {:.6f} Validation Loss {:.6f}\\n'.format(epoch + 1,\n",
    "                                                             total_loss/num_steps,float(equal_num)/20000.,total_val_loss/val_num_steps)\n",
    "    with open('./lab13-2_v4.log','a') as f:\n",
    "        f.write(output_string)\n",
    "    f.close()\n",
    "    print ('Epoch {} Train Loss {:.6f} Validation Accuracy {:.6f}'.format(epoch + 1,\n",
    "                                                             total_loss/num_steps,float(equal_num)/20000.))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f06d2d6bb20>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt.restore('./checkpoints/train_vgg19/ckpt-10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-16 17:08:45.457081: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202\n",
      "2021-12-16 17:08:46.894762: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 0.989300, Validation Loss 0.009505\r"
     ]
    }
   ],
   "source": [
    "equal_num = 0\n",
    "total_val_loss = 0\n",
    "for (batch, (img_tensor, target)) in enumerate(val_dataset):\n",
    "    val_loss = 0\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims([character_to_idx['<start>']]*BATCH_SIZE, 1)\n",
    "    features = feature_extracter(img_tensor,False)\n",
    "    features = tf.reshape(features,(features.shape[0], -1, features.shape[3]))\n",
    "    features = encoder(features)\n",
    "    result = np.full((BATCH_SIZE, 1), 27)\n",
    "    for i in range(1, target.shape[1]):\n",
    "        # passing the features through the decoder\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        predicted_id = tf.argmax(predictions,axis=1).numpy()\n",
    "        val_loss += loss_function(target[:, i], predictions)\n",
    "        result = np.concatenate((result, predicted_id.reshape((BATCH_SIZE,1))), axis=1)\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "    target_array = target.numpy()\n",
    "    total_val_loss += (val_loss / int(target.shape[1]))\n",
    "    for i in range(BATCH_SIZE):\n",
    "        for j in range(max_len):\n",
    "            if result[i][j] == 28 and target_array[i][j] == 28:\n",
    "                if (result[i][1:j] == target_array[i][1:j]).all():\n",
    "                    equal_num+=1\n",
    "                break\n",
    "    print ('Validation Accuracy {:.6f}, Validation Loss {:.6f}'.format(float(equal_num)/((batch+1)*BATCH_SIZE),total_val_loss/(batch+1)),end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_name = []\n",
    "\n",
    "for i in range(120000,140000):\n",
    "    test_img_name.append('a'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_image(image_name):\n",
    "    img = tf.io.read_file(IMAGE_DIR + image_name + '.png')\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (300, 160))\n",
    "    img = img/255 - 1.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_img_name)\n",
    "test_dataset = test_dataset.map(load_test_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.prefetch(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (None, 300, 160, 3), types: tf.float32>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=0\n",
    "for batch, img_tensor in enumerate(test_dataset):\n",
    "    hidden = decoder.reset_state(batch_size=BATCH_SIZE)\n",
    "    dec_input = tf.expand_dims([character_to_idx['<start>']]*BATCH_SIZE, 1)\n",
    "    features = feature_extracter(img_tensor,False)\n",
    "    features = tf.reshape(features,(features.shape[0], -1, features.shape[3]))\n",
    "    features = encoder(features)\n",
    "    result = np.full((BATCH_SIZE, 1), 27)\n",
    "    for i in range(1, max_len):\n",
    "        # passing the features through the decoder\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        predicted_id = tf.argmax(predictions,axis=1).numpy()\n",
    "        result = np.concatenate((result, predicted_id.reshape((BATCH_SIZE,1))), axis=1)\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "    for i in range(BATCH_SIZE):\n",
    "        output_str = ''\n",
    "        num = num+1\n",
    "        hit = False\n",
    "        for j in range(1,max_len):\n",
    "            if result[i][j] == 28:\n",
    "                hit = True\n",
    "                break\n",
    "            else:\n",
    "                output_str = output_str + idx_to_character[result[i][j]]\n",
    "\n",
    "        with open('./Lab13-2_110062401.txt','a') as f:\n",
    "            f.write('a' + str(119999 + num) + ' ' + output_str+'\\n')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Report\n",
    "\n",
    "- 一開始使用了idx_to_charactor和charactor_to_idx來做annotation， 長度是26個字母\n",
    "- 這裡中間直接使用了vgg19來做feature extractor， weight都是重train的，所以這部分是CNN_encoder。\n",
    "- RNN_decoder就用回助教給的範例code，沒有更動\n",
    "- 就結果來看，其實還蠻不錯的，vgg簡單暴力就有0.98的acc，可以的話可以是看看DenseNet也許會有更好的效果。\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
